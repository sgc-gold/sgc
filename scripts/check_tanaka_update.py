import requests
from bs4 import BeautifulSoup
import hashlib
import os
import subprocess
from datetime import datetime

# --- è¨­å®š ---
URL = "https://gold.tanaka.co.jp/commodity/souba/english/index.php"
HASH_PATH = "tanaka_hash.txt"
UPDATE_SCRIPT = "scripts/update_tanaka.py"

def load_last_hash():
    if os.path.exists(HASH_PATH):
        with open(HASH_PATH, "r", encoding="utf-8") as f:
            return f.read().strip()
    return None

def save_hash(h):
    with open(HASH_PATH, "w", encoding="utf-8") as f:
        f.write(h)

def fetch_current_hash():
    try:
        res = requests.get(URL, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        h3 = soup.select_one("h3 span")
        if h3 is None:
            print("âš  <h3> spanã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        text = h3.text.strip()
        return hashlib.md5(text.encode("utf-8")).hexdigest(), text
    except Exception as e:
        print(f"âš  ãƒãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None

def main():
    last_hash = load_last_hash()
    current_hash, current_text = fetch_current_hash()
    if current_hash is None:
        print("âš  ç¾åœ¨ã®ãƒãƒƒã‚·ãƒ¥ãŒå–å¾—ã§ããšçµ‚äº†")
        return

    print(f"ğŸ“… ç¾åœ¨ã®å…¬è¡¨æ™‚åˆ»: {current_text}")
    if last_hash == current_hash:
        print("â¸ ãƒãƒƒã‚·ãƒ¥ã«å¤‰åŒ–ãªã— â†’ æ›´æ–°ã‚¹ã‚­ãƒƒãƒ—")
        return

    print("âœ… ãƒãƒƒã‚·ãƒ¥ãŒå¤‰åŒ– â†’ ãƒ‡ãƒ¼ã‚¿æ›´æ–°é–‹å§‹")
    try:
        subprocess.run(["python", UPDATE_SCRIPT], check=True)
        save_hash(current_hash)
        print("ğŸ’¾ ãƒãƒƒã‚·ãƒ¥æ›´æ–°å®Œäº†")
    except Exception as e:
        print(f"âš  æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
